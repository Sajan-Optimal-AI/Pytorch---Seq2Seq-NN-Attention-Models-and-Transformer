{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch 4: Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation-GRU-NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pem30IMUI8wv"
      },
      "source": [
        "1.Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upV40YEQ7szP"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuqDcggGKFAP"
      },
      "source": [
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "# Set a random seed for deterministic results/reproducability."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RB5sl6tJOZgm",
        "outputId": "b80ef5d5-c1c2-42de-bc4c-49124c17512d"
      },
      "source": [
        "!python -m spacy download de_core_news_sm"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: de_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz#egg=de_core_news_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (56.1.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr0AutheaPZj"
      },
      "source": [
        "2.Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMHk5HLDN8Rr"
      },
      "source": [
        "# Instantiate German and English spaCy models.\n",
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySdztNzEOoAG"
      },
      "source": [
        "# To Tokenizes English and german text from a string into a list of strings (tokens):\n",
        "def tokenize_de(text):\n",
        "  return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "  return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Km-UDfVO3kg"
      },
      "source": [
        "# To set Field for assigning parameters for Data Preprocessing:\n",
        "SRC = Field(tokenize = tokenize_de,                         # initialize token as <sos> i.e start of sentence\n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>',                                   # eos_token as <eos> i.e end of sentence\n",
        "            lower = True)                                          # Lower case\n",
        "\n",
        "TRG= Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTw5DIZfPBxf",
        "outputId": "6f7f6c0f-7cee-403b-ce61-85cf55c6012b"
      },
      "source": [
        "# Data Split:\n",
        "train_data, validation_data, test_data = Multi30k.splits(exts=('.de','.en'),\n",
        "                                                         fields = (SRC,TRG))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:00<00:00, 1.49MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 229kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 216kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhppaxuMRgSH",
        "outputId": "5a6ebf02-563d-4b99-84d6-2c3f94f1fdf1"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data.examples)}\")                          #length of train_data i.e src len or trg len\n",
        "print(f\"Number of validation examples: {len(validation_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 29000\n",
            "Number of validation examples: 1014\n",
            "Number of testing examples: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anmgFNl3RjUl",
        "outputId": "ba55d31d-628d-42db-d1c3-b86d7de7be39"
      },
      "source": [
        "print(vars(train_data.examples[1500]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'src': ['eine', 'person', 'in', 'einer', 'warnweste', ',', 'die', 'einen', 'lkw', 'schiebt', '.'], 'trg': ['a', 'person', 'in', 'a', 'safety', 'vest', 'pushing', 'a', 'truck', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE0HjHDTRog-"
      },
      "source": [
        "# Buliding Vocabulary \n",
        "SRC.build_vocab(train_data, min_freq = 2)            # It is essential vocabulary must be build from training set  and not the validation/test set to avoid information leakage in to the model. \n",
        "TRG.build_vocab(train_data, min_freq = 2)           # Using the min_freq argument, only allow tokens that appear at least 2 times in training set"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrNaZq_NRtUl",
        "outputId": "4dfdd743-9bb3-48de-b0fb-381d6f43fe0c"
      },
      "source": [
        "print(f\"Unique tokens in source (German) vocabulary: {len(SRC.vocab)}\")               # input_dim to encoder\n",
        "print(f\"Unique tokens in target (English) vocabulary: {len(TRG.vocab)}\")              # output_dim to decoder"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in source (German) vocabulary: 7855\n",
            "Unique tokens in target (English) vocabulary: 5893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MiIsHIpRxOU"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMut7EN0R1gl"
      },
      "source": [
        "#BucketIterator :The final step of preparing the data is to create the iterators. These can be iterated on to return a batch of data which will have the PyTorch tensors containing a batch of numericalized src and trg sentences. \n",
        "#Numericalized is just a fancy way of saying they have been converted from a sequence of readable tokens to a sequence of corresponding indexes, using the vocabulary.\n",
        "BATCH_SIZE = 128\n",
        "train_iterator, valid_iterator , test_iterator = BucketIterator.splits((train_data,validation_data,test_data),batch_size = BATCH_SIZE, device = device,sort_within_batch=True,)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RC0x1bSnR7rX",
        "outputId": "ab20473d-3906-4fd4-bf97-6fa02abde81c"
      },
      "source": [
        "len(train_iterator),len(valid_iterator),len(test_iterator)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(227, 8, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Anz20RGcam8E"
      },
      "source": [
        "3.Introduction to GRU\n",
        "\n",
        "a.Note about the GRU is that it only requires and returns a hidden state, there is no cell state like in the LSTM.\n",
        "\n",
        "b.A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by Cho, et al. (2014). It combines the forget and input gates into a single “update gate.” It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.\n",
        "\n",
        "c.We also don't pass the dropout as an argument to the GRU as that dropout is used between each layer of a multi-layered RNN. As we only have a single layer, PyTorch will display a warning if we try and use pass a dropout value to it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-8xLx7reuji"
      },
      "source": [
        "4.Seq2Seq (Single layer) GRU Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTjOVFkzeDGL"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hid_dim = hid_dim                                             # Define hidden_state\n",
        "    self.embedding = nn.Embedding(input_dim,emb_dim)                   # Embedding layer\n",
        "    self.rnn = nn.GRU(emb_dim,hid_dim)                                 # no dropout as single layer GRU is used\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, src):                                              \n",
        "                                                                      # src = [src len, batch size]\n",
        "    embedded = self.dropout(self.embedding(src))                      # embedded = [src len, batch size, emb dim]\n",
        "                                                                      #outputs = [src len, batch size, hid dim * n directions] #hidden = [n layers * n directions, batch size, hid dim]\n",
        "    outputs, (hidden) = self.rnn(embedded)                            #no cell state\n",
        "\n",
        "    return hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
        "    super(). __init__()\n",
        "\n",
        "    self.hid_dim = hid_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "    self.embedding = nn.Embedding(output_dim,emb_dim)\n",
        "    \n",
        "    self.rnn = nn.GRU(emb_dim+hid_dim, hid_dim)\n",
        "\n",
        "    self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        " \n",
        "  def forward(self, input, hidden, context):\n",
        "    \n",
        "    input = input.unsqueeze(0)\n",
        "\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "    emb_con = torch.cat((embedded,context),dim=2)\n",
        "\n",
        "    output,hidden = self.rnn(emb_con, hidden)\n",
        "\n",
        "    output=torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), dim=1)\n",
        "\n",
        "    prediction = self.fc_out(output)\n",
        "\n",
        "    return prediction, hidden\n",
        "# Decoder unit consist of one embedding layer , one GRU layer and one dense layer(linear).Embedding layer is getting input of target token and that can be converted into dense representation with emb_dim.\n",
        "# GRU layer is getting input of emb_dim and two hid_dim i.e context hidden state and previous hidden state.So emb_dim is concatenated with hid_dim of context before get into GRU cell of all time step.\n",
        "#Linear layer is where output token is getting predicted receiving new hidden state i.e hid_dim*2 from GRU cell , hid_dim from context and emb_dim of target token from embedding layer. \n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "\n",
        "    assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "        \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        " \n",
        "  def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "\n",
        "    batch_size = trg.shape[1]                                     #As trg.shape is [trg len , batch_size]\n",
        "    trg_len = trg.shape[0]\n",
        "    trg_vocab_size = self.decoder.output_dim                      #len(en.vocab)\n",
        "\n",
        "    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)    #tensor to store decoder outputs # simply prediction token.\n",
        "\n",
        "    context = self.encoder(src)            #last hidden state of the encoder is context\n",
        "\n",
        "    hidden = context                       #context also used as the initial hidden state of the decoder\n",
        "\n",
        "    input = trg[0,:]                       #first input to the decoder is the <sos> tokens\n",
        "\n",
        "    for t in range(1, trg_len):\n",
        "     \n",
        "      output, hidden = self.decoder(input, hidden, context)    #insert input token embedding, previous hidden state and the context state  #receive output tensor (predictions) and new hidden state\n",
        "\n",
        "      outputs[t] = output                                      #place predictions in a tensor holding predictions for each token\n",
        "\n",
        "      teacher_force = random.random() < teacher_forcing_ratio  #decide if we are going to use teacher forcing or not\n",
        "      \n",
        "      top1 = output.argmax(1)                                  #get the highest predicted token from our predictions\n",
        "\n",
        "      input = trg[t] if teacher_force else top1                #if teacher forcing, use actual next token as next input #if not, use predicted token\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siM6_qAtUg5T"
      },
      "source": [
        "5.Training parameters of Seq2Seq GRU model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhDGVN00UH-b"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM,ENC_EMB_DIM, HID_DIM,ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM,DEC_EMB_DIM,HID_DIM,DEC_DROPOUT)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sct-k8fmUdjh",
        "outputId": "71d87214-d98a-46eb-9297-ada8e6bbadc8"
      },
      "source": [
        "#Weight initialization:\n",
        "#The parameters are initialized from a normal distribution with a mean of 0 and a standard deviation of 0.01\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(7855, 256)\n",
              "    (rnn): GRU(256, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(5893, 256)\n",
              "    (rnn): GRU(768, 512)\n",
              "    (fc_out): Linear(in_features=1280, out_features=5893, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fvDKpAlak5l",
        "outputId": "7d1e7372-b595-4e14-bd74-e70b5864d210"
      },
      "source": [
        "#Even though we only have a single layer RNN for our encoder and decoder we actually have more parameters than the last model. This is due to the increased size of the inputs to the GRU and the linear layer. \n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 14,220,293 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe4KqzE-a8Cx"
      },
      "source": [
        "#Optimizer\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBXIV-JcbBbC"
      },
      "source": [
        "# initialize the loss function, making sure to ignore the loss on <pad> tokens\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J57rzo1-bNoZ"
      },
      "source": [
        "6.Create Training and Evaluation loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F36hFe-ubMA4"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()                         #set the model into training mode\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src                   #get the source and target sentences from the batch\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()            #zero the gradients calculated from the last batch\n",
        "        \n",
        "        output = model(src, trg)         #feed the source and target into the model to get the output\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)    #has to eliminate fist element in both target and output tensor as it is <sos> and 0 respectively.Using'view' it can be done\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)              #apply loss function\n",
        "        \n",
        "        loss.backward()                            #calculate the loss while back propagation\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) #clip is used  to prevent gradients from exploding (a common issue in RNNs)\n",
        "        \n",
        "        optimizer.step()    #update the parameters of our model by doing an optimizer step\n",
        "        \n",
        "        epoch_loss += loss.item()  #sum the loss value to a running total\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKV9nnOQereB"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()   #set the mode to evaluation. This will turn off dropout \n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():  #use the with torch.no_grad() block to ensure no gradients are calculated within the block. This reduces memory consumption and speeds things up \n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg, 0)      # 0 indicates turn off teacher forcing  \n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Oqg1shrfUts"
      },
      "source": [
        "#create a function that tell that how long an epoch takes.\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzTkNamyfV6Q",
        "outputId": "aec2309a-8bff-450f-81e6-30ec2f2fef94"
      },
      "source": [
        "#At each epoch, we'll be checking if our model has achieved the best validation loss so far. If it has, we'll update our best validation loss and save the parameters of our model (called state_dict in PyTorch).\n",
        "#Then, when we come to test our model, we'll use the saved parameters used to achieve the best validation loss.\n",
        "\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model,train_iterator,optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'Pytorch Enc-Dec Model-GRU.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "# Perplexity is the multiplicative inverse of the probability assigned to the test set by the language model, normalized by the number of words in the test set. If a language model can predict unseen words from the test set.\n",
        "#As a result, better language models will have lower perplexity values or higher probability values for a test set."
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 13m 43s\n",
            "\tTrain Loss: 2.676 | Train PPL:  14.525\n",
            "\t Val. Loss: 3.637 |  Val. PPL:  37.972\n",
            "Epoch: 02 | Time: 13m 43s\n",
            "\tTrain Loss: 2.412 | Train PPL:  11.155\n",
            "\t Val. Loss: 3.569 |  Val. PPL:  35.498\n",
            "Epoch: 03 | Time: 13m 39s\n",
            "\tTrain Loss: 2.196 | Train PPL:   8.986\n",
            "\t Val. Loss: 3.559 |  Val. PPL:  35.138\n",
            "Epoch: 04 | Time: 13m 42s\n",
            "\tTrain Loss: 2.028 | Train PPL:   7.595\n",
            "\t Val. Loss: 3.559 |  Val. PPL:  35.120\n",
            "Epoch: 05 | Time: 13m 40s\n",
            "\tTrain Loss: 1.877 | Train PPL:   6.537\n",
            "\t Val. Loss: 3.621 |  Val. PPL:  37.370\n",
            "Epoch: 06 | Time: 13m 39s\n",
            "\tTrain Loss: 1.757 | Train PPL:   5.798\n",
            "\t Val. Loss: 3.616 |  Val. PPL:  37.191\n",
            "Epoch: 07 | Time: 13m 42s\n",
            "\tTrain Loss: 1.673 | Train PPL:   5.327\n",
            "\t Val. Loss: 3.596 |  Val. PPL:  36.445\n",
            "Epoch: 08 | Time: 13m 43s\n",
            "\tTrain Loss: 1.557 | Train PPL:   4.746\n",
            "\t Val. Loss: 3.669 |  Val. PPL:  39.228\n",
            "Epoch: 09 | Time: 13m 48s\n",
            "\tTrain Loss: 1.463 | Train PPL:   4.321\n",
            "\t Val. Loss: 3.669 |  Val. PPL:  39.204\n",
            "Epoch: 10 | Time: 13m 42s\n",
            "\tTrain Loss: 1.398 | Train PPL:   4.049\n",
            "\t Val. Loss: 3.760 |  Val. PPL:  42.955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-vt9vKLfhko",
        "outputId": "103e87f7-2496-47c4-b23c-fc58797ebfb7"
      },
      "source": [
        "#We'll load the parameters (state_dict) that gave our model the best validation loss and run it the model on the test set.\n",
        "model.load_state_dict(torch.load('Pytorch Enc-Dec Model-GRU.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 3.479 | Test PPL:  32.420 |\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}